{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedKKhalaf/NLP/blob/main/Selected_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6ZhybwPlRsg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.metrics.scores import f_measure, precision, recall\n",
        "import collections\n",
        "\n",
        "import re\n",
        "from itertools import islice\n",
        "\n",
        "def load_tsv(data_file, n):\n",
        "    data_features = list()\n",
        "    data = list()\n",
        "    infile = open(data_file, encoding='utf-8')\n",
        "    for line in infile:\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        label, text = line.split('\\t')\n",
        "        text_features = process_text(text, n)\n",
        "        if text_features:\n",
        "            data_features += text_features\n",
        "            data.append((text_features, label))\n",
        "    return data, data_features\n",
        "\n",
        "def process_text(text, n=1,\n",
        "                 remove_vowel_marks=False,\n",
        "                 remove_repeated_chars=False,\n",
        "                 ):\n",
        "    clean_text = text\n",
        "    if remove_vowel_marks:\n",
        "        clean_text = remove_diacritics(clean_text)\n",
        "    if remove_repeated_chars:\n",
        "        clean_text = remove_repeating_char(clean_text)\n",
        "\n",
        "    if n == 1:\n",
        "        return clean_text.split()\n",
        "    else:\n",
        "        tokens = clean_text.split()\n",
        "        grams = tokens\n",
        "        for i in range(2, n + 1):\n",
        "            grams += [  ' '.join(g) for g in list(window(tokens, i))  ]\n",
        "        return grams\n",
        "\n",
        "\n",
        "\n",
        "def window(words_seq, n):\n",
        "    \"\"\"Returns a sliding window (of width n) over data from the iterable\"\"\"\n",
        "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
        "    it = iter(words_seq)\n",
        "    result = tuple(islice(it, n))\n",
        "    if len(result) == n:\n",
        "        yield result\n",
        "    for elem in it:\n",
        "        result = result[1:] + (elem,)\n",
        "        yield result\n",
        "\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    # return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # keep 2 repeat\n",
        "\n",
        "def document_features(document, corpus_features):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in corpus_features:\n",
        "        features['has({})'.format(word)] = (word in document_words)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "Ch0FQF1QlRsk",
        "outputId": "5e54728e-b489-4b7b-f10b-8160b3c511b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data files\n",
            "train file (pos) train_Arabic_tweets_positive_20190413.tsv\n",
            "train file (neg) train_Arabic_tweets_negative_20190413.tsv\n",
            "test file (pos) test_Arabic_tweets_positive_20190413.tsv\n",
            "test file (neg) test_Arabic_tweets_negative_20190413.tsv\n"
          ]
        }
      ],
      "source": [
        "pos_train_file = 'train_Arabic_tweets_positive_20190413.tsv'\n",
        "neg_train_file = 'train_Arabic_tweets_negative_20190413.tsv'\n",
        "\n",
        "pos_test_file = 'test_Arabic_tweets_positive_20190413.tsv'\n",
        "neg_test_file = 'test_Arabic_tweets_negative_20190413.tsv'\n",
        "print('data files')\n",
        "print('train file (pos)', pos_train_file)\n",
        "print('train file (neg)', neg_train_file)\n",
        "print('test file (pos)', pos_test_file)\n",
        "print('test file (neg)', neg_test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKmtU-uYlRso",
        "outputId": "f6b158af-ad92-4227-e2a1-5da5f96ba830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading train data ....\n",
            "loading test data ....\n"
          ]
        }
      ],
      "source": [
        "n = 1\n",
        "print('loading train data ....')\n",
        "pos_train_data, pos_train_feat = load_tsv(pos_train_file, n)\n",
        "neg_train_data, neg_train_feat = load_tsv(neg_train_file, n)\n",
        "print('loading test data ....')\n",
        "pos_test_data, pos_test_feat = load_tsv(pos_test_file, n)\n",
        "neg_test_data, neg_test_feat = load_tsv(neg_test_file, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATlg5mgllRsp"
      },
      "source": [
        "# Training data information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzwmkSCNlRsp",
        "outputId": "b4e3176f-c66d-4275-adb6-dc688e1e7eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data info\n",
            "train data size 47000\n",
            "# of positive 23879\n",
            "# of negative 23121\n"
          ]
        }
      ],
      "source": [
        "print('train data info')\n",
        "train_data = pos_train_data + neg_train_data\n",
        "print('train data size', len(train_data))\n",
        "print('# of positive', len(pos_train_data))\n",
        "print('# of negative', len(neg_train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtaXgmZAlRsq"
      },
      "source": [
        "# Sample training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckCVQ_srlRsq",
        "outputId": "bc4a14a6-477d-4033-9e0f-c562759c3322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 random tweets .... \n",
            "(['اشراقة', '/', 'لن', 'تغلبنا', 'الدنيا', 'ونحن', 'نملك', 'قلوبا', 'نعاهد', 'ﷲ', 'صباحا', 'ومساء', 'بقولنا', ':{', 'إياك', 'نعبد', 'وإياك', 'نستعين', '}', 'طبتم', 'وطاب', 'يومكم', '🌹'], 'pos')\n",
            "(['ذلحين', 'نفسيتي', 'محتاجه', 'ترويقه', 'يلا', 'شوف', 'لي', 'حل', 'اروق', '😢'], 'neg')\n",
            "(['ٰ', 'مشش', 'معقول', '😨'], 'neg')\n",
            "(['لا', 'نحتاجك', 'ي', 'سطحي', 'ي', 'مقرف', '😷'], 'neg')\n",
            "(['\\u200cسر', 'المشاهير', 'بوزن', 'مثالي', 'معنا', 'رشاقتي', '🤗هي', 'البدايه', 'الصحيه', 'لجسم', 'سليم', 'وقوام', 'رش☺يق', 'الدايت', 'الاول', 'حمية', 'كلين…'], 'neg')\n",
            "(['الله', 'يرحم', 'ميو', 'بن', 'ميو', 'الشرازي', '💔'], 'neg')\n",
            "(['وكيل', 'الشيطان', '#قطر', 'لماذا', '#الدوحه', 'و', 'في', 'عهد', '#تنظيم_الحمدين', 'دعمها', 'للمعاض', 'السعودي', 'الاماراتي', 'الموريتاني', 'الليبي', 'الصومالي', 'ا…'], 'neg')\n",
            "(['ظهور', 'راقصه', 'عاريه', 'تماما', 'في', 'احدى', 'فيديوهات', 'الفرق', 'الكوريه', '😱', 'شاهد', 'قبل', 'الحذف', '!!🔥🔥'], 'neg')\n",
            "(['الله', 'يابن', 'زنان', 'المفروض', 'انت', 'اخر', 'واحد', 'تتكلم', 'لانك', 'انت', 'بالذات', 'قلبتها', 'متعه', 'طال', 'عمرك', '😂', 'انتهت', 'فترة', 'فرحتكم', 'بالصداره…'], 'pos')\n",
            "(['تنزيل', '#الاحداثيات', 'في', '#مابز', '📌', 'Maps.', 'me', '📌📌', 'في', 'ثواني', '📌', '..👍', '☑️☑️☑️☑️☑️☑️☑️☑️', '(', 'يستخدم', 'بدون', 'انترنت', ')', 'وضع', '✈️', 'تم', 'اض…'], 'pos')\n",
            "(['_⛔', '#مناحل_ابو_سلطان', '.', '.', '🍯', 'اقسم', 'بالله', 'انه', 'عسل', '🍯', '.', '.', '.', '⛔', 'سدر', 'طبيعي', '♨️', '.', '.', '⚠️', 'مضمون…'], 'neg')\n",
            "(['-', 'أمي', '؟', 'نقها', 'يالله', 'من', 'وجع', 'الحياة', '،كف', 'عنها', 'تلك', 'الدنيا', 'الفانية', 'قوها', 'وأصرف', 'عنها', 'ما', 'يؤذي', 'قلبها', '،', 'ي', 'الله', '💟'], 'pos')\n",
            "(['تقول', 'لكل', 'انسان', 'نهاية', 'بعدين', 'تقول', 'طغاة', 'عاشوا.', 'كونك', 'تحب', 'تاخذ', 'الامور', 'حرفيا', 'كيف', 'تناقض', 'نفسك', '😂'], 'pos')\n",
            "(['ردو', 'على', 'تغريدته', 'بضحكة', 'طويلة', '😂'], 'neg')\n",
            "(['لما', 'حدا', 'يهديك', 'هيك', 'اغنية', 'هلق', 'انا', 'ما', 'حدا', 'هداني', 'ياها', 'بس', 'انو', 'شعور', 'حلو', 'بتخايل', '😍'], 'pos')\n",
            "(['كان', 'صحيتي', 'بعد', 'هدف', 'التعزيز', '😓'], 'neg')\n",
            "(['بمناسبة', 'إن', 'سمانا', 'وحدة', 'وإن', 'المطر', 'اللي', 'يلمس', 'يديني', 'هو', 'نفسه', 'اللي', 'يبل', 'كتفك..أحب', 'اقول', 'ب', 'إن', 'يا', 'بخته', 'يا', 'بختته', 'والله', ':('], 'neg')\n",
            "(['حبي', 'الاولي', 'وخسرته', '😴'], 'neg')\n",
            "(['أسأل', 'الله', 'العظيم', 'أن', 'يجعلكم', 'من', 'أسعدالسعداء.', 'وأن', 'يتم', 'عليكم', 'الصحةوالشفاء', '.', 'وان', 'يحفظكم', 'من', 'كل', 'مكروه.', 'ويعطاكم', 'ما', 'ت…'], 'pos')\n",
            "(['#معصيتي_راحتي', 'اهخ', 'غرقنني', 'مسسعبل', '😭'], 'neg')\n",
            "(['.', 'مايرخص', 'إلا', 'اللي', 'تربى', 'على', 'الرخص', 'وأهل', 'الفضيله', 'يعشقون', 'الفضيله', 'أحيان', 'سبة', 'شخص', 'تترك', 'كذا', 'شخص', 'وأحيان', 'سبة', 'شخص', 'تغلي', 'قبيله', '❤'], 'pos')\n",
            "(['بس', 'وصلت', 'متأخرة', 'لأن', 'مديري', 'زنخ', 'وما', 'خلاني', 'اسمع', 'اول', 'دقيقة', '😪'], 'neg')\n",
            "(['طيب', 'هو', 'اذا', 'زار', 'مدرسه', 'يدخل', 'الفصول', 'ويصور', 'وشوفوني', 'اشتغل', 'واسال', '🌚'], 'neg')\n",
            "(['تنفسوا', 'الصباح', 'حبا', 'ورضا', 'فلا', 'شيء', 'أروع', 'من', 'قلوب', 'باتت', 'راضية', 'وأصبحت', 'مبتسمة', '..', 'صباح', 'الابتسامة', '😊', '.', '.'], 'pos')\n",
            "(['اللهم', '❤', 'دبر', 'المستقبل', 'الذي', 'نجهله', 'ويسر', 'الغد', 'الذي', 'يقلقنا', 'وقرب', 'الحلم', 'الذي', 'نرقبه', 'وهب', 'لنا', 'أملا', 'يملؤنا.♡', 'اللهم', '❤', 'توفيقا…'], 'pos')\n",
            "(['☜', '#الساعه', ':00🕛', '☞', 'تم', 'تجديد', 'رصيد', 'الاعضاء', 'لكل', 'عضو', 'تغريدات', 'باليوم', '📍', '📌#أجباري_وليس_أختياري📍', '🌟┈┉┅━❀❀━┅┉🌟', '#Gp_30', '☟', '☟…'], 'neg')\n",
            "(['ربنا', 'ياخدهم', '💔'], 'neg')\n",
            "(['الخيل', 'لو', 'تصهل', 'بالمواقف', 'عنيده', 'موجود', 'فارسها', '..', 'وكارب', 'رسنها', 'مثل', 'شاعر', 'يعسف', 'البيوت', 'الفريده', 'لاجاب', 'جزلات', 'المعاني', '..', 'وزن…'], 'pos')\n",
            "(['الحمد', 'لله💙', 'ألف', 'مبروك', 'يازعماء', '💙', 'القادم', 'أجمل', 'إن', 'شاء', 'الله', '🙏🏼'], 'pos')\n",
            "(['اشهر', 'وبضعة', 'ايام', 'اعجتني', 'كلمة', 'وبضعة', 'ايام', 'ترى', 'جلست', 'ساعه', 'علشان', 'اكتبها', 'تعالي', 'خذي', 'الزرقاء', 'ام', 'جود'], 'pos')\n",
            "(['مؤشر', 'خطير', 'جدا', 'ذياده', 'معدل', 'الجريمة', 'وهي', 'القتل🔫🔪💣', 'والاغتصاب', 'والزني', '📻', 'نقرأها', 'كل', 'يوم.', 'البلاء', 'لايأتي', 'بقوم', 'الا', 'بذنب.', 'الله…'], 'neg')\n",
            "(['مثل', 'مقال', 'حسين', '👌'], 'pos')\n",
            "(['صباح', 'الخير..', 'والله', 'لو', 'تسجني', 'ب', 'صدرك', 'اللي', 'لو', 'تحضنته', '،', 'أمنت', 'بإن', 'القيود', 'أحيان', 'حرية', ':('], 'neg')\n",
            "(['الي', 'عنده', 'خبر', 'يفيدها', 'ياعالميين', '💛'], 'pos')\n",
            "(['قوتنا', 'بعد', 'الله', 'دائما', 'أنتم🌹💙', 'الف', 'مبرووك', '💙', 'الحمد', 'لله', 'و', 'الشكر', 'له', '🙏'], 'pos')\n",
            "(['ٲليس', 'من', 'العيب', 'ان', 'تحفظ', 'كل', 'الأغاني', '..', 'وتكرر', 'نفس', 'السورة', 'في', 'كل', 'صلاة', '..', '💔'], 'neg')\n",
            "(['أنسام', 'ماتت', '!!', 'يا', 'وجع', 'القلب', 'كل', 'يوم', 'كنت', 'بدخل', 'من', 'على', 'اخبارها', 'من', 'العيال', 'وببقول', 'هتبقي', 'كويسة', 'وربنا', 'هيخيب', 'ظني', 'ومش', 'هقرا…'], 'neg')\n",
            "(['مو', 'أم', '🤔'], 'neg')\n",
            "(['\\u200cصباح_الخير', 'صباح', 'ھادئ', '🍃', 'يعآنق', 'السماء', 'ب', 'امنيات', 'مسبوقه', 'ب', 'كلمة', 'يآرب.💕🍃', '🌷🌷'], 'pos')\n",
            "(['بس', 'ريال', '😐'], 'neg')\n",
            "(['مونتاج', 'زامل', '((رئيس', 'الشهداء))', '☆', 'أداء/', '#فرقة_الشهيد_القائد', '☆', 'كلمات/', '#عاقل_بن_صبر', 'مونتاج/', 'معاذ', 'العامري…'], 'pos')\n",
            "(['اعتقد', 'لأ', '..', 'لأنه', 'في', 'حفظ', 'مفيش', 'فهم.', 'أنا', 'مره', 'سألت', 'واحد', 'مغربي', 'صديق', 'عن', 'الموضوع', 'ده', 'قالي', 'أنه…'], 'pos')\n",
            "(['😞', 'وترى', 'مسار', 'حياتي', 'بذمتك', '☹️'], 'neg')\n",
            "(['وكنت', 'أظن', 'الشوق', 'في', 'البعد', 'وحده', '..', 'ولم', 'أدر', 'أن', 'الشوق', 'في', 'البعد', 'والقرب', '!', '❥'], 'pos')\n",
            "(['روزي', 'و', 'جيسو', 'معا', 'في', 'الكوتشيلا', 'اليوم', '💗', '|'], 'pos')\n",
            "(['مو', 'عندي', '🌚'], 'neg')\n",
            "(['بمناسبة', 'فوز', 'الهلال', '..', '💙', 'سحب', 'على', 'آيفون', 'XR📱', 'رتويت', 'وتابع', '-', 'السحب', 'بعد', 'ساعة', 'موثق', 'بالفديو', '💪'], 'pos')\n",
            "(['يعل', 'هو', 'الجعل', '؟؟', '🐜'], 'neg')\n",
            "(['مع', 'الأسف', 'الثقة', '💔'], 'neg')\n",
            "(['روع', 'المسلمين', '😁'], 'pos')\n",
            "(['امسح', 'يا', 'محمود', 'الصوره', 'بتاعتك', 'امسح', 'الاول', '😂'], 'pos')\n",
            "(['#وش_يقول_الليل', 'يازين', 'النوم', 'بعد', 'العشاء😍👌😂😂', 'والحمدلله', 'أصبحنا', '💓👌', '#صباح_الخير', '🌸'], 'pos')\n",
            "(['اهه', 'لو', 'ترجعع', 'صدق', '💔'], 'neg')\n",
            "(['اللهم', 'صل', 'على', 'محمد', 'وعلى', 'آل', 'محمد', 'كما', 'صليت', 'على', 'إبراهيم', 'وعلى', 'آل', 'إبراهيم', 'إنك', 'حميد', 'مج…'], 'pos')\n",
            "(['حاجة', 'بقت', 'بيض', 'ع', 'الصبح', '😕'], 'neg')\n",
            "(['قال', 'لها', ':', 'انتي', 'بتستاهلي', 'واحد', 'احسن', 'مني', '💔', '-', 'مسحت', 'دموعها', 'وقالت', ':', 'هات', 'رقموا', '🌚', '!', 'لا', 'احساس', 'ولا', 'ضمير', '🐸💔😂'], 'neg')\n",
            "(['تخيلي', 'إني', 'عشت', 'لا', 'لي', 'ولا', 'علي', 'ولايهمني', 'لا', 'الرايح', 'ولا', 'الجاي..بينما', 'معك', 'حتى', 'إلتفاتااتك', 'تهمني', ':('], 'neg')\n",
            "(['اول', 'mvp', 'من', 'الشرق', 'من', 'بعد', 'ليبرون', 'في', 'الهيت', '😭'], 'neg')\n",
            "(['عسىٰ', 'قبرك', 'أمتلت', 'زواياه', 'مطر', 'يا', '#أبوي', '💔', 'الله', 'يرحمكك', '💔🤕', '#مطر_الرياض'], 'neg')\n",
            "(['مساء', 'الخير', '😍', 'حابين', 'نذكركم', 'بالمسابقة', 'و', 'السحب', 'على', 'الجائزة', 'باذن', 'الله', 'نهاية', 'الأسبوع', '(', 'السبت', ')', '💞', 'السؤال:', 'ماهي', 'بلورة', 'العسل', '🤔', 'بانتظار', 'إجاباتكم', '😍🙏💞'], 'neg')\n",
            "(['لا', 'هييك', 'كتيير', '.،', 'في', 'اشي', 'غلط', '.،'], 'neg')\n",
            "(['الآحترآم', 'ليس', 'صآلح', 'للإستخدآم', 'مع', 'كل', 'المخلوقآت', 'ف', 'بعضهم', 'يعششق', 'الإهآنة', 'لآ', 'أكثر', '✋'], 'neg')\n",
            "(['ملييون', 'ب24', 'ساعه', '😭', 'كملوا', 'سترريم🔥', '#BoyWithLuv70M'], 'neg')\n",
            "(['سويه', 'مثل', 'بيكهون', '(:', 'اوكي', 'الاسم', 'غلط', '🌚'], 'neg')\n",
            "(['اللهم', 'اشرح', 'لي', 'صدري', '😔'], 'neg')\n",
            "(['الخيال', 'ممتاز', 'لاتتهوري', '😀'], 'pos')\n",
            "(['صباح', 'الخير', '🌸'], 'pos')\n",
            "(['💥', 'اللهم', 'اكفني', 'بحلالك', 'عن', 'حرامك', 'وأغنني', 'بفضلك', 'عمن', 'سواك.', '..', '💥', 'اللهم', 'لاسهل', 'إ…'], 'neg')\n",
            "(['#يوم_الجمعه', '~~', '#اجازة', 'وراحة', 'من', '#الرتويت', 'الاجباري', 'والتبادل', '#ودي', 'بين', 'الاعضاء', 'من', '#الساعه', 'صباح', 'يوم', 'ا…'], 'pos')\n",
            "(['مربربه', 'او', 'متزوجة', 'او', 'كبيره', 'اتمنى', 'الاضافه', 'Bbm', 'E3B3199B', 'Kik', 'r7al3030', 'فديتك', '💔'], 'neg')\n",
            "(['⏳اقل', 'من', 'ساعه', 'على', 'بداية', 'الكلاسيكو', '.', '-', 'نسبة', 'تفاؤلك', '🤔', '؟'], 'neg')\n",
            "(['ردة', 'فعل', 'لينا', 'اختي', 'لما', 'صحت', 'وشافت', 'ميك', 'اب', 'امي', 'وهي', 'طالعه', '😂'], 'pos')\n",
            "(['الله', 'يطيب', 'ايامك', 'ومنك', 'نتعلم', 'طال', 'عمرك', '🌸'], 'pos')\n",
            "(['تعال', 'انت', 'والوتيوتر', 'وتغدو', 'وما', 'بدي', 'حدا', 'يفداني', '😷', 'خلصووني', 'بس', 'من', 'ام', 'الدهن', '.كمان', 'اوزعلكم', 'الدهن', 'الي', 'بالفريزر', 'كمان', '😷😷'], 'neg')\n",
            "(['فخوره', 'ببناتي', '😭', '#BLACKPINKxCoachella_D1'], 'pos')\n",
            "(['ستريم', 'ستريم', 'نبغى', 'مليون', ':('], 'neg')\n",
            "(['من', 'أجمل', 'ماكتب', 'الشاعر', 'أحمدالمحياوي:', 'من', 'العدل', 'لا', 'كرهت', 'انسان', 'تبعد', 'عن', 'الذم', 'والتشويه', 'يمكن', 'إذا', 'جربوه', 'يبان', 'إ…'], 'pos')\n",
            "(['هه', 'بموتت', '😭'], 'neg')\n",
            "(['شيئان', 'يحددان', 'من', 'أنت', '؟', 'صبرك,', 'عندما', 'لاتملك', 'شيء', '..', 'وأخلاقك,', 'عندما', 'تملك', 'كل', 'شيء', '..', 'أنت', '\"رائع\"', 'حين', 'تتجاهل', 'من', 'يسيء', 'إليك…'], 'neg')\n",
            "(['وش', 'يهدي', 'الشوق', 'والدنيا', 'مطر', ':('], 'neg')\n",
            "(['فاهمين', 'غلط', 'يقصد', 'وين', 'دور', 'المرأة', 'في', 'هذه', 'المسخرة', 'ليش', 'ابو', 'رغد', 'الرغدين', 'الي', 'يهز', 'ماهي', 'ام', 'رغد', 'مثلا', '😄', 'Feminism', 'رياضي', 'اجتماعي', 'ثقافي'], 'pos')\n",
            "(['من', '✋', 'نجحت', '😂'], 'neg')\n",
            "(['⠀♩', 'ماذا', 'أكتب', 'وانت', 'من', 'أعطى', 'الوصف', 'ومن', 'أين', 'تأتيني', 'الحروف', 'وأنت', 'سيدها', 'وكاتب', 'سطورها', 'وكأني', 'بواحة', 'غناءة', 'من', 'الكلمات', 'ت…'], 'pos')\n",
            "(['وكنهم', 'ابهات', 'المستقبل', 'عادي', '😵'], 'neg')\n",
            "(['مؤلم', '💔', '..'], 'neg')\n",
            "(['🌴', 'غراس', 'الخير', '🌴', 'أتدري', 'كيف', 'يسرق', 'عمر', 'المرء', '،', 'بأن', 'يشغل', 'عن', 'يومه', 'في', 'ارتقاب', 'غده', '،', 'ولا', 'يزال', 'كذلك', 'حتى', 'ينقضي', 'أجله', 'ويده', 'خ…'], 'pos')\n",
            "(['فيه', 'سيارة', 'تحت', 'تريله', 'ليه؟؟!', 'قاعده', 'ترضع', '🍼🍼', '^__^', 'أمزح', 'أمزح', 'حادث', '💔', '😂😂🚬', '😂😂', '.', '.', 'هه'], 'neg')\n",
            "(['الفيديو', 'دا', 'فيه', 'كمية', 'جمال', 'غير', 'عادى', '💙'], 'pos')\n",
            "(['أكبر', 'أكذوبه', 'أنك', 'تقنع', 'نفسك', 'أنك', 'قوي', '😞'], 'neg')\n",
            "(['خلينا', 'نتأمل', 'ي', 'علا', '😑'], 'neg')\n",
            "(['#زلزل_الملعب_نصرنا_بيلعب', 'ياللي', 'غلاك', 'بداخلي', 'ماله', 'ابعاد', 'تبطي', 'ولا', 'حد', 'من', 'خفوقي', 'يزيلك.', '💛أحبك', 'يانصر', 'و', 'الله', 'احبك', '💛', 'فالن…'], 'pos')\n",
            "(['اللهم', 'اتم', 'علينا', ',', 'نعمتك', 'وعافيتك', 'وسترك', 'في', 'الدنيا', 'والآخرة', '.', 'اللهم', 'صل', 'و', 'سلم', 'على', 'نبينا', 'محمد', '♡.', 'مساء', 'الخير', '💚'], 'pos')\n",
            "([\"''\", 'استغفر', 'الله', 'عدد', 'خلقه', 'ورضا', 'نفسه', 'وزنه', 'عرشه', 'ومداد', 'كلماته', '💜'], 'pos')\n",
            "(['#الهلال_الاهلي', '#الاتحاد_النصر', 'يالهلال', 'أنت', 'مكانك', 'في', 'الصداره', 'انت', 'الأول', 'دايم', 'وغيرك', 'يهيم', 'تدري', 'وش', 'جمهورك', 'الوافي', 'شعاره…'], 'pos')\n",
            "(['ممكن', 'م', 'ننسى', 'اعلاميين', 'السودان', 'الخذلونا', 'والقنوات', 'الكان', 'جنها', 'اكل', 'ورقص', ':)'], 'pos')\n",
            "(['اول', 'مره', 'اسمع', '🙂'], 'pos')\n",
            "(['..', 'في', 'داخلي', 'طفلة', 'لا', 'تود', 'ان', 'تگبر', 'وفي', 'واقعي', 'أنثى', 'لا', 'تقبل', 'ان', 'تضعف', 'وفي', 'احلامي', 'حلم', 'أبيض', 'لن', 'أقبل', 'ان', 'يصبح', 'أسودا', 'وفي', 'أحس…'], 'pos')\n",
            "(['منتبه', 'له', 'لو', 'ما', 'بينا', 'حكي', ':('], 'neg')\n",
            "(['يارب', 'باكر', 'اجازة', '💔'], 'neg')\n",
            "(['نقعد', 'مش', 'ديوتي', 'والقسم', 'كله', 'يتصل', 'بيا', 'ع', 'الصبح', '😒😒', 'وتخرب', 'النومة', '🌚', 'شن', 'استفدت', 'اني', 'توا', '😭😭'], 'neg')\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "sample_size = 100\n",
        "print('{} random tweets .... '.format(sample_size))\n",
        "for s in random.sample(train_data, sample_size):\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuAFinMNlRsr",
        "outputId": "6b60b214-5726-4d3b-86d9-e6d51abb2f0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data info\n",
            "test data size 47000\n",
            "positive: 5970\n",
            "negative: 5781\n"
          ]
        }
      ],
      "source": [
        "print('test data info')\n",
        "test_data = pos_test_data + neg_test_data\n",
        "print('test data size', len(train_data))\n",
        "print('positive:', len(pos_test_data))\n",
        "print('negative:', len(neg_test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1bHVdBXlRss",
        "outputId": "fefaa70a-9b52-43de-9b48-7ad8dbde77e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of features: 770508\n"
          ]
        }
      ],
      "source": [
        "#merging features\n",
        "all_features = pos_train_feat + neg_train_feat + \\\n",
        "               pos_test_feat + pos_test_feat\n",
        "print('length of features:', len(all_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIzpl1_dlRst",
        "outputId": "7e497ab2-19ae-4449-ba82-d320df3781b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 sample features ...\n",
            "['تتفرج', 'وانته', ':(', 'و', 'فالدنيا', '#حرم_السفير', 'ولابق…', 'يا', 'المفرطة', 'الاخطاء', 'هي', 'ايامها', 'لاعب', 'اشوف', 'حراام', '😂', 'الغيره', 'ال', 'الزاخرة', 'يشجعون', 'البملا', 'تونا', 'القلب', 'كان', 'على', 'السنين', 'جيفآرا', 'أحبها', 'الله', 'الجو', 'سحب', 'ستيشن', 'وهو', 'مقاومه', 'الخير', 'أجلك', '🌹', 'قو', 'ما', '.', '🌷', 'اليومي', 'تقريبا', 'هبوط', 'هاي', 'لك', 'من', 'والتي', 'هذا', 'تسكت', 'ويمجدوا', 'تثبت', 'التاق', '#ڪبريآء_آميرﮩ', 'في', 'قوتنا', 'يشتغل', 'صيااح', 'ما', '.', 'يارب', '.!', 'اهديت', 'إذا', 'سأكمل', '#مسابقه', 'خطيبة', 'قسم', 'يومهم', '👍🏻', 'عديل', '💙', 'على', 'الفشل', 'لكل', 'محمد', '..', '#النصر_الاتحاد', 'بعطائك..', '..', 'بنت', 'قلب', '..', 'من', 'نطلع', 'جبت', 'كيف', 'مساح', 'مرتين', '👌', 'درجه', 'من', '.', 'ما', 'رقصتنا\"', 'العطرة', 'فيه', 'تاثير', 'لاتسأل', 'لما']\n"
          ]
        }
      ],
      "source": [
        "print('{} sample features ...'.format(sample_size))\n",
        "print(random.sample(all_features, sample_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMOmuXqelRsu"
      },
      "outputs": [],
      "source": [
        "#frequencies\n",
        "\n",
        "all_features_count = {}\n",
        "for w in all_features:\n",
        "    all_features_count[w] = all_features_count.get(w, 0) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsAqP2K8lRsv",
        "outputId": "aaa350ec-afe0-4849-fce3-c8b9c5f152b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample frequencies\n",
            "[('وهلت', 7), ('تل…', 1), ('وانشقاقات', 10), ('ترقصص', 1), ('عطها', 1), ('يلازمنا', 1), ('أدبا', 3), ('ابطني', 1), ('❤ك', 1), ('السنقل', 1), ('تبوق', 1), ('فاهمتك', 1), ('ارسله', 2), ('السعيده', 7), ('عرفتني', 1), ('مربع', 2), ('حرمان…', 4), ('يالهوي', 1), ('يحرقون', 1), ('الجيش', 23), ('القوى', 4), ('الرتوته', 2), ('تمام؟', 2), ('طرد', 107), ('الإعلامية،النادي', 8), ('العلم؛…', 2), ('بدلا…', 4), ('اعطتك', 1), ('Star', 1), ('❄', 5)]\n",
            "freq of word في is 9550\n",
            "freq of word فى is 220\n",
            "freq of word من is 12655\n"
          ]
        }
      ],
      "source": [
        "#sample frequencies\n",
        "print('sample frequencies')\n",
        "print(random.sample(list(all_features_count.items()), 30))\n",
        "word = 'في'\n",
        "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\n",
        "word = 'فى'\n",
        "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\n",
        "word = 'من'\n",
        "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3OqqGlUlRsw",
        "outputId": "0e1688ba-cf52-456e-f98c-7ad19690990e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training data: 47000\n",
            "min document frequency: 47\n",
            "max document frequency: 46060\n"
          ]
        }
      ],
      "source": [
        "#threshold\n",
        "print('size of training data:',  len(train_data))\n",
        "min_df = int(0.001 * len(train_data))\n",
        "max_df = int(0.98 * len(train_data))\n",
        "print('min document frequency:', min_df)\n",
        "print('max document frequency:', max_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t71EuROxlRsy",
        "outputId": "764bef7e-b99a-4663-9ab6-e0cbc740eaae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1961 are kept out of 770508\n"
          ]
        }
      ],
      "source": [
        "# remove features that have frequency below/above the threshold\n",
        "my_features = set([word for word, freq in all_features_count.items() if  max_df > freq > min_df ])\n",
        "print(len(my_features), 'are kept out of', len(all_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRIChy11lRsz",
        "outputId": "14f85a89-bec7-42ee-b07e-187ccd9065c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 sample of selected features:\n",
            "['نبي', 'الجميلة', 'يلا', 'لي', 'حتي', 'كذا', 'الثاني', 'يصلون', 'لله', 'ونحن', 'الجمعة', 'دع', 'أجل', 'ياللي', 'الا', 'ع', 'وغايات', 'لابد', 'الأغاني', 'بنشكر', '✴️', 'دول', 'زرع', 'ليه', 'يخيب', 'البشير', '🍯', 'غدا', 'ينام', 'الجديد', 'فترة', 'لاحول', 'تماما', 'بعض', 'ضمن', 'بما', 'بل', 'سألوا', 'اصبحنا', 'لتغريداتك؟!', 'توجيه', 'مندس', 'فيك', 'النفس', 'لقاء', 'الدولية', 'يستاهل', 'مباراة', '👍🏻', 'اكتب', 'وشافت', 'ويوم', 'الي', 'فيها', 'لم', 'خلف', 'الجمال', 'فلا', 'برج', 'بها', 'اتوقع', 'تجاوزنا', '😫', 'نسينا', '↴', 'مالي', 'ﻭﻓﻲ', 'صلوا', 'ماوحشتك', 'ماحنا', 'لمن', 'واللي', ':', 'اشوف', 'بدري', 'عشق', 'عنك', 'تلفزيونية', 'الجاهل', 'ﷺ', 'كن', 'سيدنا', 'ذكر', '♪', 'القدر', 'لحدي', 'المرأة', 'العروس', 'مره', '↓', '🥀', 'تستاهل', 'الأهلي', '||', 'الحق', 'كانوا', 'زعماء', 'تجد', 'يقدر', 'اجل']\n"
          ]
        }
      ],
      "source": [
        "print('{} sample of selected features:'.format(sample_size))\n",
        "print(random.sample(list(my_features), sample_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCLymYF_lRs0"
      },
      "source": [
        "# generating features for training documents ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drxAhBC4lRs0"
      },
      "outputs": [],
      "source": [
        "feature_sets = [(document_features(d, my_features), c) for (d, c) in train_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLmYFl7_lRs0"
      },
      "source": [
        "# training ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3scidqRllRs1",
        "outputId": "fe94d9c8-59c2-4231-89d9-50cbc0e7c70e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training is done\n"
          ]
        }
      ],
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(feature_sets)\n",
        "print('training is done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCyWiJ7MlRs1"
      },
      "source": [
        "# Most informative features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRaLYK3ilRs1",
        "outputId": "71f6b031-6ce3-4e9f-bce2-0b24e6989739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "               has(موثق) = True              pos : neg    =    238.5 : 1.0\n",
            "                  has(😭) = True              neg : pos    =    202.0 : 1.0\n",
            "                  has(😢) = True              neg : pos    =    171.3 : 1.0\n",
            "            has(المسيار) = True              pos : neg    =    170.1 : 1.0\n",
            "              has(وصلوا) = True              pos : neg    =    166.9 : 1.0\n",
            "                  has(😳) = True              neg : pos    =    164.2 : 1.0\n",
            "             has(الشروط) = True              pos : neg    =    151.4 : 1.0\n",
            "              has(وتابع) = True              pos : neg    =    143.9 : 1.0\n",
            "               has(ببكي) = True              neg : pos    =    143.6 : 1.0\n",
            "                  has(🥀) = True              neg : pos    =    132.4 : 1.0\n",
            "              has(السحب) = True              pos : neg    =    118.4 : 1.0\n",
            "                  has(💐) = True              pos : neg    =    116.5 : 1.0\n",
            "             has(العروس) = True              neg : pos    =    113.3 : 1.0\n",
            "             has(يشجعون) = True              pos : neg    =    100.5 : 1.0\n",
            "               has(ركبت) = True              neg : pos    =    100.2 : 1.0\n",
            "                has(بضل) = True              neg : pos    =     98.8 : 1.0\n",
            "               has(نشبة) = True              neg : pos    =     97.4 : 1.0\n",
            "                  has(😞) = True              neg : pos    =     97.3 : 1.0\n",
            "                  has(🙈) = True              neg : pos    =     97.2 : 1.0\n",
            "             has(وكأنهم) = True              neg : pos    =     94.7 : 1.0\n",
            "                  has(😐) = True              neg : pos    =     91.9 : 1.0\n",
            "                  has(😔) = True              neg : pos    =     91.3 : 1.0\n",
            "                 has(:() = True              neg : pos    =     87.5 : 1.0\n",
            "                  has(😷) = True              neg : pos    =     85.0 : 1.0\n",
            "              has(شديدة) = True              neg : pos    =     82.4 : 1.0\n",
            "            has(الزرقاء) = True              pos : neg    =     82.3 : 1.0\n",
            "              has(توجيه) = True              neg : pos    =     79.5 : 1.0\n",
            "              has(سودان) = True              neg : pos    =     78.8 : 1.0\n",
            "                 has(فض) = True              neg : pos    =     78.1 : 1.0\n",
            "            has(بمناسبة) = True              pos : neg    =     74.9 : 1.0\n",
            "                  has(😱) = True              neg : pos    =     71.3 : 1.0\n",
            "               has(انظر) = True              pos : neg    =     70.7 : 1.0\n",
            "                  has(🤔) = True              neg : pos    =     67.5 : 1.0\n",
            "              has(قيمته) = True              pos : neg    =     67.5 : 1.0\n",
            "                  has(😒) = True              neg : pos    =     66.7 : 1.0\n",
            "                  has(😕) = True              neg : pos    =     66.0 : 1.0\n",
            "            has(الايفون) = True              pos : neg    =     62.9 : 1.0\n",
            "               has(مبلغ) = True              pos : neg    =     62.1 : 1.0\n",
            "           has(المسابقة) = True              pos : neg    =     61.6 : 1.0\n",
            "                  has(💗) = True              pos : neg    =     61.0 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier.show_most_informative_features(40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ6JylIUlRs2"
      },
      "source": [
        "# generating features for test documents ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmzF6LfElRs2"
      },
      "outputs": [],
      "source": [
        "test_features = [(document_features(d, my_features), c) for (d, c) in test_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C1qsYlklRs2"
      },
      "source": [
        "# classify test instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it47M0ctlRs2"
      },
      "outputs": [],
      "source": [
        "ref_sets = collections.defaultdict(set)\n",
        "test_sets = collections.defaultdict(set)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_features):\n",
        "    ref_sets[label].add(i)\n",
        "    observed = classifier.classify(feats)\n",
        "    test_sets[observed].add(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kihIqJlGlRs3"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2StGbYv6lRs3",
        "outputId": "ccff7a1a-a21f-4958-cec9-b11b776053dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:  0.8913283975831844\n",
            "pos precision:  0.9198425478618716\n",
            "pos recall: 0.8611390284757119\n",
            "neg precision:  0.8654657578708211\n",
            "neg recall: 0.9225047569624633\n",
            "positive f-score: 0.8895233151656718\n",
            "negative f-score: 0.8930754416813196\n"
          ]
        }
      ],
      "source": [
        "print('accuracy: ', nltk.classify.accuracy(classifier, test_features))\n",
        "print('pos precision: ', precision(ref_sets['pos'], test_sets['pos']))\n",
        "print('pos recall:', recall(ref_sets['pos'], test_sets['pos']))\n",
        "print('neg precision: ', precision(ref_sets['neg'], test_sets['neg']))\n",
        "print('neg recall:', recall(ref_sets['neg'], test_sets['neg']))\n",
        "print('positive f-score:', f_measure(ref_sets['pos'], test_sets['pos']))\n",
        "print('negative f-score:', f_measure(ref_sets['neg'], test_sets['neg']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}